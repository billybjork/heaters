# Production Deployment Guide

## Overview

Heaters deploys on Render using Docker containers, with PostgreSQL, AWS S3/CloudFront for media storage, and Oban for background job processing. Clip playback uses pre-generated temp files via FFmpeg stream copy — there is no on-demand FFmpeg streaming in the request path.

## Architecture

```
Browser → CloudFront (signed URLs) → S3 Proxy Files
                                          ↑
Phoenix App (Render) → Oban Workers → FFmpeg stream copy → temp clips / exported clips
       ↓
  PostgreSQL (Render)
```

**Key Technologies:**
- **Application**: Phoenix LiveView on Render (Docker)
- **CDN/Storage**: AWS S3 + CloudFront (presigned URLs for proxy files)
- **Database**: PostgreSQL 16 on Render with pgvector
- **Background Jobs**: Oban with queue-based worker separation
- **Media Processing**: FFmpeg stream copy from CloudFront URLs (no local downloads)

## Render Service Configuration

Defined in `render.yaml`:

- **phoenix-web** (web service): Handles HTTP traffic, runs `default` Oban queue (cron scheduler, light jobs). Does NOT run heavy media processing queues.
- **app-db** (PostgreSQL 16): Primary database with pgvector extension.

### Oban Queue Architecture

Production queues are configured via the `OBAN_QUEUES` environment variable:

- **Web service** (`OBAN_QUEUES=default`): Runs cron scheduler (Dispatcher every minute, CleanupWorker every 4 hours)
- **Worker service** (if separated): Runs `media_processing`, `exports`, `temp_clips`, `background_jobs`, `maintenance` queues

See `config/runtime.exs` for dynamic queue configuration.

### Required Environment Variables

| Variable | Description |
|----------|-------------|
| `DATABASE_URL` | PostgreSQL connection string (from Render) |
| `SECRET_KEY_BASE` | Phoenix secret key |
| `PHX_HOST` | Public hostname for URL generation |
| `APP_ENV` | `production` |
| `OBAN_QUEUES` | Comma-separated queue names to run |
| `PROD_S3_BUCKET_NAME` | S3 bucket for media storage |
| `PROD_CLOUDFRONT_DOMAIN` | CloudFront distribution domain |
| AWS credentials | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` |

## Playback Cache (Production)

Temp clips are generated by `PlaybackCache.Worker` on the `:temp_clips` Oban queue:
- FFmpeg stream-copies segments from CloudFront proxy URLs to local `/tmp/clip_{id}_{timestamp}.mp4`
- `CleanupWorker` runs every 4 hours: expires files >15 min old, enforces 1GB LRU limit, monitors disk space (500MB threshold)
- Configuration in `config/config.exs` under `:heaters, :playback_cache`

## Deployment Checklist

### Pre-Deployment
- [ ] FFmpeg available in Docker image
- [ ] Environment variables configured in Render
- [ ] S3 bucket and CloudFront distribution created
- [ ] Database migrations applied

### Post-Deployment
- [ ] Verify health check endpoint responds
- [ ] Confirm Oban cron jobs are scheduling (check logs for Dispatcher)
- [ ] Test clip playback in review interface
- [ ] Monitor Oban job queues for failures
- [ ] Verify S3/CloudFront connectivity

## Troubleshooting

**Clip playback not working:**
- Check Oban `:temp_clips` queue is running on the correct service
- Verify proxy files exist in S3 for the source video
- Review `PlaybackCache.Worker` logs for FFmpeg errors

**Oban jobs not running:**
- Confirm `OBAN_QUEUES` env var includes the needed queues
- Check that the `default` queue service is running (required for cron scheduling)

**Disk space issues:**
- `CleanupWorker` logs cache statistics every 4 hours
- Adjust `:playback_cache` config if temp files accumulate

---

## Planned Migration: Fly.io + FLAME

The target architecture replaces Render with Fly.io and uses the FLAME pattern
(Fleeting Lambda Application for Modular Execution) for elastic, scale-to-zero
compute on media processing workloads.

### What is FLAME?

FLAME (`{:flame, "~> 0.5"}`) boots a **full copy of your application** on an
ephemeral Fly.io Machine, sends a function closure to it via Erlang distribution,
and tears the machine down when idle. Unlike traditional FaaS, the remote node
has your entire app context — Repo, PubSub, config — so closures Just Work.

Key characteristics:
- ~3 second cold-start on Fly.io (boots your Docker image via Machines API)
- `FLAME.Terminator` monitors parent connection; auto-destroys on disconnect
- Configurable pools with `min`/`max` runners, concurrency, and idle timeouts
- `FLAME.LocalBackend` for dev/test (executes closures in-process)

### Target Architecture

```
Browser → CloudFront → S3 Proxy Files
                            ↑
Phoenix App (Fly.io) → Oban Jobs → FLAME.call(Runner, fn -> FFmpeg work end)
       ↓                                ↓
  PostgreSQL (Fly.io)        Ephemeral Fly Machine (same Docker image)
                             └─ boots, runs FFmpeg, uploads to S3, shuts down
```

### Integration Pattern

Oban continues to own job durability, retries, scheduling, and observability.
FLAME wraps the expensive compute inside the worker's `perform/1`:

```elixir
# In an Oban worker
def perform(%Oban.Job{args: %{"clip_id" => clip_id}}) do
  clip = Repo.get!(Clip, clip_id) |> Repo.preload(:source_video)

  FLAME.call(Heaters.FFmpegRunner, fn ->
    # This closure runs on an ephemeral Fly Machine.
    # Repo, PubSub, and all app config are available.
    TempClip.build(clip)
  end)
end
```

### Configuration

```elixir
# mix.exs
{:flame, "~> 0.5"}

# config/runtime.exs (prod only)
config :flame, :backend, FLAME.FlyBackend
config :flame, FLAME.FlyBackend,
  token: System.fetch_env!("FLY_API_TOKEN"),
  env: %{
    "DATABASE_URL" => System.fetch_env!("DATABASE_URL"),
    "SECRET_KEY_BASE" => System.fetch_env!("SECRET_KEY_BASE"),
    "POOL_SIZE" => "1"
  }

# application.ex — add pool to supervision tree
{FLAME.Pool,
 name: Heaters.FFmpegRunner,
 min: 0,           # scale to zero when idle
 max: 5,           # max concurrent machines
 max_concurrency: 2,  # concurrent jobs per machine
 idle_shutdown_after: 30_000}  # 30s idle before teardown
```

Key details:
- Set `min: 0` for scale-to-zero (cost: ~3s cold start on first call)
- Set `min: 1` to keep a warm runner and eliminate cold starts
- FLAME children should skip Oban and the HTTP endpoint (use `FLAME.Parent.get()` to detect)
- Reduce DB `pool_size` to 1 on FLAME children to conserve connections
- Environment variables are NOT inherited — forward them explicitly via `:env`

### Migration Steps

1. Add `{:flame, "~> 0.5"}` to deps
2. Deploy to Fly.io with FFmpeg in the Docker image
3. Configure `FLAME.FlyBackend` in `runtime.exs`
4. Add `FLAME.Pool` to the supervision tree (skip Oban/Endpoint on FLAME children)
5. Wrap heavy Oban workers (`PlaybackCache.Worker`, export workers) with `FLAME.call`
6. Use `FLAME.LocalBackend` in dev/test (closures run locally, no infrastructure needed)
7. Migrate PostgreSQL to Fly.io Postgres or keep external
