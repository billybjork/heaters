# V-JEPA 2 Embedding Pipeline

## Overview

V-JEPA 2 (Video Joint-Embedding Predictive Architecture 2) is Meta's self-supervised video encoder released June 2025. It achieves state-of-the-art on motion understanding tasks (77.3% top-1 on Something-Something v2, 39.7 R@5 on Epic-Kitchens-100 action anticipation) and is trained on 1M+ hours of internet video with no labels. It is a strong candidate as the frozen backbone encoder for the heaters contrastive sorting / embedding pipeline (trick2vec / spot2vec).

**References:**
- [Paper](https://arxiv.org/abs/2506.09985)
- [GitHub](https://github.com/facebookresearch/vjepa2)
- [Meta page](https://ai.meta.com/vjepa/)
- [HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/vjepa2)

---

## Integration with Heaters Architecture

### What Already Exists

The codebase has a **production-ready embedding pipeline** that V-JEPA 2 can plug into:

| Component | Location | Status |
|-----------|----------|--------|
| Embedding schema (pgvector) | `Processing.Embed.Embedding` | ✅ Ready |
| Embedding worker | `Processing.Embed.Worker` | ✅ Ready (needs video input) |
| Vector similarity search | `Processing.Embed.Search` | ✅ Ready |
| Python runner infrastructure | `Processing.Support.PythonRunner` | ✅ Ready |
| Python embed task | `py/tasks/embed.py` | ✅ Ready (needs V-JEPA 2 loader) |
| Pipeline config | `Pipeline.Config` | ✅ Ready (add new defaults) |
| ANN index | pgvector `<=>` operator | ✅ Ready |
| Contrastive sorting UI | — | ❌ Not yet built |
| Triplet extraction & projection head training | — | ❌ Not yet built |

### Critical Architecture Change: Keyframes → Video

**Current pipeline** (CLIP/DINOv2):
```
Clip → Keyframe extraction (JPEGs) → Python embed task → image embeddings → pgvector
```

**V-JEPA 2 pipeline** (video encoder):
```
Clip → Python embed task receives clip video path → 64-frame sampling → V-JEPA 2 → video embedding → pgvector
```

V-JEPA 2 is fundamentally a **video model** that processes 3D spatiotemporal tubelets (2 frames × 16×16 pixels). It cannot be fed static keyframe images — it requires a sequence of frames to understand motion. This is a feature: the motion trajectory *is* the signal for distinguishing tricks.

**Options for video input:**

1. **Pass clip video path** (recommended): The exported clip MP4 is already generated by the Export stage. Pass `clip.clip_filepath` (S3 path or pre-fetched local path via `TempCache.get_or_download/2`) to Python.

2. **Pass temporal frame sequence**: Extract N frames (64+) as a batch and pass paths. More complex, similar to current keyframe flow but higher frame count.

Option 1 is simpler and aligns better with V-JEPA 2's design — it can handle video files directly.

### Generation Strategy Enum

Add new strategies to `Processing.Embed.Embedding`:

```elixir
# Current strategies (image-based)
:keyframe_multi_avg
:keyframe_multi
:keyframe_single

# New strategies (video-based, for V-JEPA 2)
:video_full_clip       # Embed entire clip (up to 64 frames uniformly sampled)
:video_centered_64     # 64 frames centered on clip midpoint
:video_trick_window    # 64 frames centered on trick initiation/apex (requires metadata)
```

### Worker Changes

`Processing.Embed.Worker` currently passes `image_paths` to Python. For V-JEPA 2:

```elixir
# Current (CLIP/DINOv2)
args = %{
  clip_id: clip.id,
  image_paths: keyframe_paths,  # List of local JPEG paths
  model_name: model_name,
  generation_strategy: strategy
}

# V-JEPA 2 (video-based)
args = %{
  clip_id: clip.id,
  video_path: video_path,       # Local path to exported clip MP4
  model_name: model_name,
  generation_strategy: strategy,
  num_frames: 64                # V-JEPA 2 default
}
```

The worker can detect strategy type and branch accordingly, or we can add a separate `VideoEmbeddingWorker`.

### Python Task Extension

`py/tasks/embed.py` needs a V-JEPA 2 model loader:

```python
# Model loading (add to existing loader logic)
if "vjepa" in model_name.lower() or "v-jepa" in model_name.lower():
    from transformers import AutoModel, AutoProcessor
    processor = AutoProcessor.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device).eval()
    model_type_str = "v_jepa2"

def run_video_embed(clip_id, video_path, model_name, generation_strategy,
                    num_frames=64, device="cpu", **kwargs):
    """Embed a video clip using V-JEPA 2."""
    # 1. Load video, sample frames
    # 2. Process through V-JEPA 2
    # 3. Pool temporal embeddings to single vector
    # 4. Return embedding + metadata
```

### Pipeline Config Defaults

In `Pipeline.Config`:

```elixir
# Existing defaults
default_embedding_model: "openai/clip-vit-base-patch32"
default_embedding_generation_strategy: "keyframe_multi_avg"

# V-JEPA 2 alternative
# default_embedding_model: "facebook/vjepa2-vit-l"
# default_embedding_generation_strategy: "video_full_clip"
```

The backfill stage (`get_clips_needing_embeddings/0`) will automatically queue jobs for clips missing the configured default embedding variant.

---

## Why V-JEPA 2 (and not a contrastive model)

V-JEPA 2 is fundamentally **not** a contrastive learning method. It uses **masked latent feature prediction** — predicting abstract representations of masked video regions rather than reconstructing pixels or contrasting augmented views. This is a feature, not a limitation, for the heaters pipeline:

1. **Richer representations than contrastive models**: Contrastive methods (CLIP, SigLIP, InternVideo) learn to map views to a single point — collapsing information. V-JEPA 2 learns to *predict* missing information in representation space, forcing it to model dynamics, motion trajectories, and spatiotemporal structure. This is exactly what matters for distinguishing tricks (rotation direction, grab type, body positioning through time).

2. **Motion understanding >> appearance matching**: V-JEPA 2 outperforms contrastive encoders specifically on motion-centric tasks. For snowboarding tricks, the motion *is* the signal. A backside 540 and a frontside 540 look nearly identical in any single frame — the difference is the spatiotemporal trajectory. V-JEPA 2 encodes this natively.

3. **Compatible with downstream contrastive fine-tuning**: The architecture is designed for frozen encoder + lightweight head. You extract V-JEPA 2 embeddings, then train *your own* contrastive projection head on top using the triplets from the sorting pipeline. The general-purpose encoder handles "what's happening in this video" while the projection head learns "is this the same trick as that one."

4. **Self-supervised = no label dependency at base level**: Trained on 1M+ hours of unlabeled video. You inherit all of that world knowledge without needing any snowboarding-specific training data at the encoder level.

### Video as Skill Encoding

A critical insight from recent world model research (Sitzmann, 2026): video doesn't just capture visual appearance or motion—it implicitly encodes **human knowledge about skills, tasks, and their structure**.

When V-JEPA 2 watches millions of hours of video, it learns:
- How humans initiate, execute, and complete skilled movements
- The temporal structure of actions (setup → execution → landing)
- Physics of rotation, momentum, and balance
- Variations in style within the same underlying skill

For snowboarding tricks, this means V-JEPA 2 likely represents a backside 540 not just as "pixels rotating" but as a *learned skill pattern* with recognizable phases. A grab isn't just hand-to-board contact—it's a timing-dependent component of the overall trick structure.

**Practical implication**: V-JEPA 2 embeddings should cluster tricks by skill similarity (same trick, different riders/spots) rather than visual similarity (same snow color, same camera angle). This is exactly what trick2vec needs.

### Model Comparison

| Model | Type | Motion understanding | Availability | Notes |
|---|---|---|---|---|
| **V-JEPA 2** | Self-supervised (JEPA) | SOTA (77.3% SSv2) | Open weights, PyTorch | Best motion features; frozen eval paradigm |
| InternVideo 2 | Vision-text contrastive | Strong (69.7% SSv2) | Open | Good but weaker on motion; relies on text supervision |
| VideoPrism | Multi-task (Google) | Strong | Restricted | Not openly available |
| VideoMAE v2 | Masked autoencoder | Good | Open | Reconstructs pixels — learns lower-level features |
| SigLIP 2 | Image-text contrastive | Weak on motion (56.4%) | Open | Primarily appearance; limited temporal reasoning |
| SlowFast | Supervised | Moderate | Open | Requires labels; older architecture |
| CLIP (current) | Image-text contrastive | Weak | Open | Image model, no temporal understanding |
| DINOv2 (current) | Self-supervised | Weak | Open | Image model, no temporal understanding |

---

## Architecture: How V-JEPA 2 Works

### Encoder
- Vision Transformer (ViT), up to ViT-g (~1B params)
- Input: video divided into 3D patches ("tubelets") of 2 frames × 16×16 pixels
- Processes unmasked tubelets → outputs contextual embeddings
- Pretrained model sizes: ViT-L (304M), ViT-H (632M), ViT-g (1B)

### Predictor
- Takes encoder output + mask tokens for missing regions
- Predicts the *representation* (not pixels) of masked tubelets
- Forces encoder to learn predictable, semantic features while discarding noise
- Key insight: by predicting in representation space, the model learns what *matters* (dynamics, semantics) and ignores what doesn't (exact pixel values, lighting variation, snow texture)

### Training
- Two stages:
    1. **Self-supervised pretraining**: 22M videos, 1M+ hours, no labels. Multi-block masking strategy where large contiguous spatiotemporal regions are masked
    2. **Action-conditioned training** (V-JEPA 2-AC): Optional stage for robotics — adds action prediction. Not needed for our use case
- Collapse prevention: EMA (exponential moving average) target encoder + stop-gradient — no contrastive or variance/covariance regularizers needed
- Loss: L1 loss between predicted and target representations in latent space

### Downstream Evaluation Pattern (relevant for us)
- **Freeze** the encoder weights entirely
- Train a **4-layer attentive probe** on top:
    - 4 transformer blocks
    - Last block uses cross-attention with learnable query token
    - Classification logits averaged across sampled clips
- This is the evaluation paradigm we'd adapt — replacing the classification probe with a contrastive projection head

---

## Proposed Architecture: V-JEPA 2 + Contrastive Sorting Pipeline

The right mental model is a **two-layer embedding system**:

```
Layer 1: V-JEPA 2 frozen encoder (general video understanding)
   ↓ high-dimensional spatiotemporal embeddings (1024-dim for ViT-L)
Layer 2: Lightweight contrastive projection head (trick2vec / spot2vec)
   ↓ domain-specific similarity space (128-256 dim)
pgvector ANN index → contrastive sorting UI → triplets → back to Layer 2
```

### How it maps to the existing pipeline

**Pipeline Stage 7 (Embeddings)** → Replace CLIP/DINOv2 with V-JEPA 2 frozen encoder
- Extract embeddings for all clips using V-JEPA 2
- These are your initial vectors for the ANN index
- V-JEPA 2's motion features should produce meaningfully better initial neighbors than image models

**New: Contrastive Sorting UI** → Human-in-the-loop refinement
- Present anchor clip, surface reference clips from pgvector similarity search
- User selects closest match (positive), skipped clips become negatives
- Active learning / uncertainty sampling to maximize information gain per interaction
- V-JEPA 2's better initial representations mean fewer sorting iterations needed

**New: Triplet Event Logging** → Append-only `clip_sorting_events` table
- All sorting events logged (anchor_clip_id, selected_positive_clip_id, rejected_clip_ids[])
- Projected into triplet dataset: (anchor_embedding, positive_embedding, negative_embeddings)

**New: Projection Head Training** → Train contrastive head, NOT the full encoder
- Lightweight MLP or small transformer head on top of frozen V-JEPA 2 features
- Trained with triplet loss (or InfoNCE / supervised contrastive loss) using human-generated triplets
- Fast to train, fast to iterate on, doesn't require massive compute
- Separate heads for trick2vec and spot2vec

**Re-embedding Loop** → Re-embed all clips through encoder + trained projection head
- New ANN index in the projected space
- Subsequent sorting rounds converge faster
- Active learning selects clips uncertain *in the projected space*

### Why Frozen Encoder + Projection Head (not full fine-tuning)

1. **Data efficiency**: You'll have hundreds, maybe low thousands of triplets initially. Full fine-tuning a 1B parameter model on that will overfit catastrophically. A projection head with ~1-10M params is the right scale.

2. **Iteration speed**: Retraining the projection head can happen frequently. The sorting → triplets → retrain → re-embed loop can run daily or on-demand.

3. **Stable base features**: The frozen encoder provides consistent, high-quality features regardless of how the projection head evolves. You can always re-project from the same base embeddings.

4. **Separate concerns**: trick2vec and spot2vec can share the same frozen V-JEPA 2 encoder but have different projection heads. The encoder computes once; the heads are cheap.

5. **Progressive enhancement**: Start frozen, move to LoRA fine-tuning of later encoder layers once you have enough data, then eventually full fine-tuning if warranted.

---

## Bitter Lesson Alignment

This architecture is explicitly aligned with [Sutton's Bitter Lesson](http://www.incompleteideas.net/IncIdea/TheBitterLesson.html):

**What you're leveraging (general methods + compute):**
- V-JEPA 2's 1M+ hours of self-supervised pretraining = pure compute leverage
- No hand-crafted features about snowboarding, tricks, rotations, or grabs baked in
- The model learned spatiotemporal dynamics from watching the world — including human motion, physics, rotation, gravity — without being told what any of it means
- The contrastive sorting pipeline is a *learning* method: it lets the embedding space discover the structure of tricks through data (human similarity judgments), not through a taxonomy you designed

**What you're NOT doing (brittle heuristics):**
- Not building a rule-based trick classifier ("if rotation > 360 AND grab == mute THEN backside_540_mute")
- Not manually defining feature hierarchies or decision trees
- Not encoding snowboarding-specific physics or trick definitions into the model
- Not relying on pose estimation as the *primary* embedding signal (though it can augment)

**Where domain knowledge enters appropriately:**
- The *structure* of the sorting pipeline (contrastive pairs, active learning, triplet generation) is a meta-method — it's the mechanism by which the model captures domain complexity, not domain knowledge itself
- Separate vectors for trick vs spot similarity is a structural choice, not a heuristic

---

## Implementation Phases

### Phase 1: V-JEPA 2 Baseline Embeddings

**Goal**: Replace CLIP/DINOv2 with V-JEPA 2, using existing infrastructure.

- [ ] Add V-JEPA 2 model loader to `py/tasks/embed.py`
- [ ] Add video-based generation strategies to `Embedding` schema enum
- [ ] Modify `EmbeddingWorker` to pass video path for V-JEPA strategies
- [ ] Add `run_video_embed()` function to Python task
- [ ] Update `Pipeline.Config` defaults to use V-JEPA 2
- [ ] Batch extract V-JEPA 2 embeddings for all existing exported clips
- [ ] Evaluate: do raw V-JEPA 2 nearest neighbors look reasonable?

**Files to modify:**
- `lib/heaters/processing/embed/embedding.ex` (enum)
- `lib/heaters/processing/embed/worker.ex` (video path support)
- `lib/heaters/pipeline/config.ex` (defaults)
- `py/tasks/embed.py` (V-JEPA 2 loader + video embedding)
- New migration for enum values

**Compute considerations:**
- ViT-L (304M params) runs on CPU but slowly (~5-10s per clip)
- GPU recommended for batch processing
- Consider FLAME wrapping for elastic GPU compute on Fly.io

### Phase 2: Contrastive Sorting UI

**Goal**: Build the human-in-the-loop sorting interface.

- [ ] Design sorting LiveView (`SortLive`)
- [ ] Implement anchor + candidates display using `EmbedSearch.similar_clips/2`
- [ ] Add keyboard shortcuts for rapid selection
- [ ] Create `clip_sorting_events` table (append-only event log)
- [ ] Implement triplet extraction from event log
- [ ] Add active learning: surface uncertain clips for next round

**New files:**
- `lib/heaters_web/live/sort_live.ex`
- `lib/heaters/sorting/event.ex` (schema)
- `lib/heaters/sorting/triplet_extraction.ex`
- Migration for `clip_sorting_events`

**Design principle: Sorting as Training Signal**

The sorting UI isn't just labeling data—it's generating **paired (perception, human_judgment) training signal**, analogous to RLHF in language models. This affects event schema design:

```elixir
# Capture rich signal, not just the selection
schema "clip_sorting_events" do
  belongs_to :anchor_clip, Clip
  belongs_to :selected_clip, Clip           # The positive
  field :rejected_clip_ids, {:array, :id}   # Hard negatives (explicitly skipped)
  field :candidate_clip_ids, {:array, :id}  # All candidates shown (for implicit negatives)
  field :sorting_mode, Ecto.Enum            # :trick_similarity, :spot_similarity
  field :session_id, :binary_id             # Group events by sorting session
  field :response_time_ms, :integer         # Faster = more confident judgment
  field :candidate_rankings, {:array, :map} # Optional: rank order if UI supports it
  timestamps()
end
```

Key signals to capture:
- **Hard negatives**: Clips explicitly passed over (stronger signal than random negatives)
- **Response time**: Faster selections may indicate more obvious differences
- **Session context**: Sequential selections within a session have dependencies
- **Sorting mode**: Same clip pair might be similar for tricks but different for spots

### Phase 3: Projection Head Training

**Goal**: Train lightweight heads to create trick2vec / spot2vec spaces.

- [ ] Design projection head architecture (2-3 layer MLP, ~1-5M params)
- [ ] Implement triplet loss training in Python
- [ ] Add `run_train_projection_head()` task
- [ ] Create `projection_heads` table to store trained weights
- [ ] Implement re-embedding through encoder + projection head
- [ ] Add separate heads for trick similarity vs spot similarity

**New files:**
- `py/tasks/train_projection.py`
- `lib/heaters/processing/embed/projection_head.ex` (schema)
- `lib/heaters/processing/embed/projected_embedding.ex` (schema for projected vectors)

### Phase 4: Active Learning Loop

**Goal**: Close the feedback loop for continuous improvement.

- [ ] Implement uncertainty sampling in projected embedding space
- [ ] Continuous retrain: new triplets → retrain head → re-project → update index
- [ ] Measure convergence: sorting iterations until neighbors stabilize
- [ ] Add metrics dashboard for embedding quality

### Phase 5: Scale Up (if warranted)

- [ ] LoRA fine-tuning of V-JEPA 2 encoder's later layers
- [ ] Upgrade to ViT-g (1B params) if ViT-L proves limiting
- [ ] Evaluate segmentation-augmented embeddings (rider masks)
- [ ] Explore multi-scale embeddings (full clip + sub-clips)

---

## Open Questions

1. **Embedding dimensionality**: V-JEPA 2 ViT-L outputs 1024-dim embeddings. Should the projection head reduce this (e.g., to 128 or 256)? Lower dims are faster for ANN search but may lose information. Start with 256, test 128 and 512.

2. **Projection head architecture**: Simple MLP vs. small transformer? MLP is simpler and likely sufficient for initial data volumes.

3. **Contrastive loss function**: Triplet loss vs. InfoNCE vs. supervised contrastive. InfoNCE handles multiple negatives per anchor more efficiently. Could start with triplet loss and migrate to InfoNCE as batch sizes grow.

4. **Frame sampling for V-JEPA 2**: Uses 64 frames. For short trick clips (2-5 seconds at 30fps = 60-150 frames), 64 frames covers most of the action. For longer clips, need to decide: subsample uniformly, or use trick keyframes to center the window?

5. **Multi-scale embeddings**: Should we embed full clips AND sub-clips (individual tricks within lines)? V-JEPA 2 can handle variable-length inputs.

6. **GPU compute**: V-JEPA 2 is significantly more compute-intensive than CLIP. Options:
   - FLAME with GPU-enabled Fly machines
   - Batch processing on external GPU (Lambda Labs, Vast.ai)
   - Modal.com for serverless GPU inference

7. **Storage for V-JEPA embeddings**: 1024-dim vectors are 4x larger than 256-dim CLIP embeddings. pgvector handles this fine, but monitor index size.

---

## Connection to C-JEPA

[C-JEPA](https://arxiv.org/abs/2410.19560) (NeurIPS 2024) demonstrated that JEPA and contrastive objectives are complementary:
- Adding VICReg (variance/invariance/covariance regularization) to JEPA prevents embedding collapse and improves convergence
- The alignment (positive pairs map nearby) and uniformity (embeddings distributed on unit hypersphere) properties from contrastive learning are preserved while JEPA's predictive representations add richer spatiotemporal structure
- Suggests that training the projection head with a contrastive loss on top of JEPA features should work well — the two paradigms reinforce each other

---

## References

- [V-JEPA 2 paper](https://arxiv.org/abs/2506.09985) - architecture, training, benchmarks
- [V-JEPA 2 code](https://github.com/facebookresearch/vjepa2) - model weights, eval scripts
- [C-JEPA](https://arxiv.org/abs/2410.19560) - connecting JEPA with contrastive SSL (NeurIPS 2024)
- [V-JEPA (v1) blog](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/) - LeCun's motivation
- [The Bitter Lesson](http://www.incompleteideas.net/IncIdea/TheBitterLesson.html) (Sutton, 2019)
- [The flavor of the bitter lesson for computer vision](https://www.vincentsitzmann.com/bitter-lesson/) (Sitzmann, 2026) - video as skill encoding, perception-action loops
